{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6626693e",
   "metadata": {
    "papermill": {
     "duration": 0.003935,
     "end_time": "2025-12-29T18:03:01.241745",
     "exception": false,
     "start_time": "2025-12-29T18:03:01.237810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LOADING PACKAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f502fc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:03:01.254340Z",
     "iopub.status.busy": "2025-12-29T18:03:01.251483Z",
     "iopub.status.idle": "2025-12-29T18:03:02.430134Z",
     "shell.execute_reply": "2025-12-29T18:03:02.428339Z"
    },
    "papermill": {
     "duration": 1.187293,
     "end_time": "2025-12-29T18:03:02.432536",
     "exception": false,
     "start_time": "2025-12-29T18:03:01.245243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘lubridate’\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    date, intersect, setdiff, union\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(dplyr)\n",
    "library(lubridate)\n",
    "library(readr)\n",
    "library(timeDate)\n",
    "library(stringr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1618362a",
   "metadata": {
    "papermill": {
     "duration": 0.00364,
     "end_time": "2025-12-29T18:03:02.439961",
     "exception": false,
     "start_time": "2025-12-29T18:03:02.436321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LOADING DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2487ab2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:03:02.483575Z",
     "iopub.status.busy": "2025-12-29T18:03:02.449983Z",
     "iopub.status.idle": "2025-12-29T18:03:48.215312Z",
     "shell.execute_reply": "2025-12-29T18:03:48.213359Z"
    },
    "papermill": {
     "duration": 45.773844,
     "end_time": "2025-12-29T18:03:48.217587",
     "exception": false,
     "start_time": "2025-12-29T18:03:02.443743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING NYC TAXI DATASET\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“\u001b[1m\u001b[22mOne or more parsing issues, call `problems()` on your data frame for details,\n",
      "e.g.:\n",
      "  dat <- vroom(...)\n",
      "  problems(dat)”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Parsing issues detected:\n",
      "  Total problematic rows: 5\n",
      "  First 5 problems:\n",
      "\u001b[90m# A tibble: 5 × 5\u001b[39m\n",
      "       row   col expected   actual    file                                      \n",
      "     \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m      \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m     \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                                     \n",
      "\u001b[90m1\u001b[39m 11\u001b[4m9\u001b[24m\u001b[4m1\u001b[24m\u001b[4m6\u001b[24m664     1 18 columns 1 columns /kaggle/input/taxi-dataset/Taxi Datset.csv\n",
      "\u001b[90m2\u001b[39m 11\u001b[4m9\u001b[24m\u001b[4m1\u001b[24m\u001b[4m6\u001b[24m665     2 18 columns 2 columns /kaggle/input/taxi-dataset/Taxi Datset.csv\n",
      "\u001b[90m3\u001b[39m 11\u001b[4m9\u001b[24m\u001b[4m1\u001b[24m\u001b[4m6\u001b[24m666     2 18 columns 2 columns /kaggle/input/taxi-dataset/Taxi Datset.csv\n",
      "\u001b[90m4\u001b[39m 11\u001b[4m9\u001b[24m\u001b[4m1\u001b[24m\u001b[4m6\u001b[24m667     1 18 columns 1 columns /kaggle/input/taxi-dataset/Taxi Datset.csv\n",
      "\u001b[90m5\u001b[39m 11\u001b[4m9\u001b[24m\u001b[4m1\u001b[24m\u001b[4m6\u001b[24m668     1 18 columns 1 columns /kaggle/input/taxi-dataset/Taxi Datset.csv\n",
      "\n",
      "  Note: These issues will be handled during data cleaning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset structure:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 11,916,667\n",
      "Columns: 18\n",
      "$ VendorID              \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"1\", \"1\", \"1\", \"1\", \"2\", \"2\", \"2\", \"2\", \"2\", \"1\"…\n",
      "$ tpep_pickup_datetime  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"01/01/2020 12:28:15 AM\", \"01/01/2020 12:35:39 A…\n",
      "$ tpep_dropoff_datetime \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"01/01/2020 12:33:03 AM\", \"01/01/2020 12:43:04 A…\n",
      "$ passenger_count       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1, 1, 1, 1, 1, 1, 1, 1, 4, 2, 2, 1, 1, 1, 1, 3, …\n",
      "$ trip_distance         \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1.20, 1.20, 0.60, 0.80, 0.00, 0.03, 0.00, 0.00, …\n",
      "$ RatecodeID            \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, …\n",
      "$ store_and_fwd_flag    \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\"…\n",
      "$ PULocationID          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 238, 239, 238, 238, 193, 7, 193, 193, 193, 246, …\n",
      "$ DOLocationID          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 239, 238, 238, 151, 193, 193, 193, 193, 193, 48,…\n",
      "$ payment_type          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, …\n",
      "$ fare_amount           \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 6.00, 7.00, 6.00, 5.50, 3.50, 2.50, 2.50, 0.01, …\n",
      "$ extra                 \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 3.0, 3.0, 3.0, 0.5, 0.5, 0.5, 0.5, 0.0, 0.5, 3.0…\n",
      "$ mta_tax               \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.0, 0.5, 0.5…\n",
      "$ tip_amount            \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1.47, 1.50, 1.00, 1.36, 0.00, 0.00, 0.01, 0.00, …\n",
      "$ tolls_amount          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n",
      "$ improvement_surcharge \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3…\n",
      "$ total_amount          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 11.27, 12.30, 10.80, 8.16, 4.80, 3.80, 3.81, 2.8…\n",
      "$ congestion_surcharge  \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 2.5, 2.5, 2.5, 0.0, 0.0, 0.0, 0.0, 2.5, 2.5, 2.5…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total records loaded: 11,916,667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns: 18\n"
     ]
    }
   ],
   "source": [
    "cat(\"\\n====================================\\n\")\n",
    "cat(\"LOADING NYC TAXI DATASET\\n\")\n",
    "cat(\"====================================\\n\\n\")\n",
    "\n",
    "# Load CSV with specified column types to avoid parsing issues\n",
    "df <- read_csv('/kaggle/input/taxi-dataset/Taxi Datset.csv', \n",
    "               show_col_types = FALSE,  # Suppress column type messages\n",
    "               col_types = cols(\n",
    "                 VendorID = col_character(),\n",
    "                 tpep_pickup_datetime = col_character(),\n",
    "                 tpep_dropoff_datetime = col_character(),\n",
    "                 store_and_fwd_flag = col_character(),\n",
    "                 .default = col_double()\n",
    "               ))\n",
    "\n",
    "# Check if there were any parsing problems\n",
    "if(nrow(problems(df)) > 0) {\n",
    "  cat(\"⚠ Parsing issues detected:\\n\")\n",
    "  cat(sprintf(\"  Total problematic rows: %s\\n\", nrow(problems(df))))\n",
    "  cat(\"  First 5 problems:\\n\")\n",
    "  print(head(problems(df), 5))\n",
    "  cat(\"\\n  Note: These issues will be handled during data cleaning\\n\\n\")\n",
    "} else {\n",
    "  cat(\"✓ Data loaded successfully with no parsing issues\\n\\n\")\n",
    "}\n",
    "\n",
    "cat(\"Initial dataset structure:\\n\")\n",
    "glimpse(df)\n",
    "cat(sprintf(\"\\nTotal records loaded: %s\\n\", format(nrow(df), big.mark=\",\")))\n",
    "cat(sprintf(\"Total columns: %s\\n\", ncol(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86fcb62",
   "metadata": {
    "papermill": {
     "duration": 0.004571,
     "end_time": "2025-12-29T18:03:48.226720",
     "exception": false,
     "start_time": "2025-12-29T18:03:48.222149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **1. DATA TYPE CONVERSION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0f5b21",
   "metadata": {
    "papermill": {
     "duration": 0.004468,
     "end_time": "2025-12-29T18:03:48.235629",
     "exception": false,
     "start_time": "2025-12-29T18:03:48.231161",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b92d66ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:03:48.248888Z",
     "iopub.status.busy": "2025-12-29T18:03:48.247220Z",
     "iopub.status.idle": "2025-12-29T18:04:25.530133Z",
     "shell.execute_reply": "2025-12-29T18:04:25.527535Z"
    },
    "papermill": {
     "duration": 37.29424,
     "end_time": "2025-12-29T18:04:25.534450",
     "exception": false,
     "start_time": "2025-12-29T18:03:48.240210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 1: DATA TYPE CONVERSION ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting columns to appropriate data types...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types after conversion:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 11,916,667\n",
      "Columns: 18\n",
      "$ VendorID              \u001b[3m\u001b[90m<fct>\u001b[39m\u001b[23m 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, …\n",
      "$ tpep_pickup_datetime  \u001b[3m\u001b[90m<dttm>\u001b[39m\u001b[23m 2020-01-01 00:28:15, 2020-01-01 00:35:39, 2020-…\n",
      "$ tpep_dropoff_datetime \u001b[3m\u001b[90m<dttm>\u001b[39m\u001b[23m 2020-01-01 00:33:03, 2020-01-01 00:43:04, 2020-…\n",
      "$ passenger_count       \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m 1, 1, 1, 1, 1, 1, 1, 1, 4, 2, 2, 1, 1, 1, 1, 3, …\n",
      "$ trip_distance         \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1.20, 1.20, 0.60, 0.80, 0.00, 0.03, 0.00, 0.00, …\n",
      "$ RatecodeID            \u001b[3m\u001b[90m<fct>\u001b[39m\u001b[23m 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, …\n",
      "$ store_and_fwd_flag    \u001b[3m\u001b[90m<fct>\u001b[39m\u001b[23m N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, …\n",
      "$ PULocationID          \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m 238, 239, 238, 238, 193, 7, 193, 193, 193, 246, …\n",
      "$ DOLocationID          \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m 239, 238, 238, 151, 193, 193, 193, 193, 193, 48,…\n",
      "$ payment_type          \u001b[3m\u001b[90m<fct>\u001b[39m\u001b[23m 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, …\n",
      "$ fare_amount           \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 6.00, 7.00, 6.00, 5.50, 3.50, 2.50, 2.50, 0.01, …\n",
      "$ extra                 \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 3.0, 3.0, 3.0, 0.5, 0.5, 0.5, 0.5, 0.0, 0.5, 3.0…\n",
      "$ mta_tax               \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.0, 0.5, 0.5…\n",
      "$ tip_amount            \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1.47, 1.50, 1.00, 1.36, 0.00, 0.00, 0.01, 0.00, …\n",
      "$ tolls_amount          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n",
      "$ improvement_surcharge \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3…\n",
      "$ total_amount          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 11.27, 12.30, 10.80, 8.16, 4.80, 3.80, 3.81, 2.8…\n",
      "$ congestion_surcharge  \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 2.5, 2.5, 2.5, 0.0, 0.0, 0.0, 0.0, 2.5, 2.5, 2.5…\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. DATA TYPE CONVERSION\n",
    "# ============================================================================\n",
    "cat('\\n=== STEP 1: DATA TYPE CONVERSION ===\\n')\n",
    "cat(\"Converting columns to appropriate data types...\\n\")\n",
    "\n",
    "# Convert dates from string format (MM/DD/YYYY HH:MM:SS AM/PM) to datetime\n",
    "df$tpep_pickup_datetime <- mdy_hms(df$tpep_pickup_datetime)\n",
    "df$tpep_dropoff_datetime <- mdy_hms(df$tpep_dropoff_datetime)\n",
    "\n",
    "# Convert categorical variables to factors for better memory usage and analysis\n",
    "df$VendorID <- as.factor(df$VendorID)             \n",
    "df$RatecodeID <- as.factor(df$RatecodeID)          \n",
    "df$store_and_fwd_flag <- as.factor(toupper(df$store_and_fwd_flag))  \n",
    "df$payment_type <- as.factor(df$payment_type)\n",
    "\n",
    "# Convert location and passenger fields to integers\n",
    "df$PULocationID <- as.integer(df$PULocationID)\n",
    "df$DOLocationID <- as.integer(df$DOLocationID)\n",
    "df$passenger_count <- as.integer(df$passenger_count)\n",
    "\n",
    "cat(\"\\nData types after conversion:\\n\")\n",
    "glimpse(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051e6726",
   "metadata": {
    "papermill": {
     "duration": 0.006358,
     "end_time": "2025-12-29T18:04:25.549288",
     "exception": false,
     "start_time": "2025-12-29T18:04:25.542930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **2. DUPLICATE REMOVAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f627abf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:04:25.568728Z",
     "iopub.status.busy": "2025-12-29T18:04:25.566211Z",
     "iopub.status.idle": "2025-12-29T18:04:33.457393Z",
     "shell.execute_reply": "2025-12-29T18:04:33.455195Z"
    },
    "papermill": {
     "duration": 7.904549,
     "end_time": "2025-12-29T18:04:33.460120",
     "exception": false,
     "start_time": "2025-12-29T18:04:25.555571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 2: DUPLICATE REMOVAL ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found: 12,949 (0.11% of dataset)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Removed 12,949 duplicate records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Remaining records: 11,903,718\n"
     ]
    }
   ],
   "source": [
    "cat(\"\\n--- STEP 2: DUPLICATE REMOVAL ---\\n\")\n",
    "\n",
    "initial_rows <- nrow(df)\n",
    "\n",
    "# Identify duplicates before removing\n",
    "n_duplicates <- nrow(df) - nrow(distinct(df))\n",
    "pct_duplicates <- round((n_duplicates / nrow(df)) * 100, 2)\n",
    "\n",
    "cat(sprintf(\"Duplicates found: %s (%.2f%% of dataset)\\n\", \n",
    "            format(n_duplicates, big.mark = \",\"),\n",
    "            pct_duplicates))\n",
    "\n",
    "# Remove exact duplicate rows\n",
    "df <- distinct(df)\n",
    "\n",
    "cat(sprintf(\"✓ Removed %s duplicate records\\n\", format(n_duplicates, big.mark=\",\")))\n",
    "cat(sprintf(\"✓ Remaining records: %s\\n\", format(nrow(df), big.mark=\",\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797eb808",
   "metadata": {
    "papermill": {
     "duration": 0.005349,
     "end_time": "2025-12-29T18:04:33.471010",
     "exception": false,
     "start_time": "2025-12-29T18:04:33.465661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **3. MISSING VALUES TREATMENT (NAs)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e22829",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:04:33.485524Z",
     "iopub.status.busy": "2025-12-29T18:04:33.483933Z",
     "iopub.status.idle": "2025-12-29T18:04:39.516211Z",
     "shell.execute_reply": "2025-12-29T18:04:39.514008Z"
    },
    "papermill": {
     "duration": 6.042635,
     "end_time": "2025-12-29T18:04:39.518919",
     "exception": false,
     "start_time": "2025-12-29T18:04:33.476284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 3: MISSING VALUES TREATMENT (NAs) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values by column:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      na_count\n",
      "VendorID                 78499\n",
      "tpep_pickup_datetime         5\n",
      "tpep_dropoff_datetime        5\n",
      "passenger_count          78504\n",
      "trip_distance                5\n",
      "RatecodeID               78504\n",
      "store_and_fwd_flag       78504\n",
      "PULocationID                 5\n",
      "DOLocationID                 5\n",
      "payment_type             78504\n",
      "fare_amount                  5\n",
      "extra                        5\n",
      "mta_tax                      5\n",
      "tip_amount                   5\n",
      "tolls_amount                 5\n",
      "improvement_surcharge        5\n",
      "total_amount                 5\n",
      "congestion_surcharge         5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total NAs: 392,580 (0.18% of all cells)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Removed 78,504 rows with missing values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Remaining records: 11,825,214\n"
     ]
    }
   ],
   "source": [
    "cat(\"\\n--- STEP 3: MISSING VALUES TREATMENT (NAs) ---\\n\")\n",
    "\n",
    "# Count NAs per column\n",
    "nas_per_column <- as.data.frame(colSums(is.na(df))) %>% \n",
    "  setNames('na_count')\n",
    "\n",
    "cat(\"\\nMissing values by column:\\n\")\n",
    "print(nas_per_column[nas_per_column$na_count > 0, , drop = FALSE])\n",
    "\n",
    "total_nas <- sum(nas_per_column$na_count)\n",
    "total_cells <- nrow(df) * ncol(df)\n",
    "pct_nas <- round((total_nas / total_cells) * 100, 2)\n",
    "\n",
    "cat(sprintf(\"\\nTotal NAs: %s (%.2f%% of all cells)\\n\", \n",
    "            format(total_nas, big.mark=\",\"), \n",
    "            pct_nas))\n",
    "\n",
    "before_na_removal <- nrow(df)\n",
    "\n",
    "# Remove all rows with any NA values\n",
    "df <- df %>% na.omit()\n",
    "\n",
    "after_na_removal <- nrow(df)\n",
    "\n",
    "cat(sprintf(\"✓ Removed %s rows with missing values\\n\", \n",
    "            format(before_na_removal - after_na_removal, big.mark=\",\")))\n",
    "cat(sprintf(\"✓ Remaining records: %s\\n\", format(nrow(df), big.mark=\",\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c24320",
   "metadata": {
    "papermill": {
     "duration": 0.005868,
     "end_time": "2025-12-29T18:04:39.530391",
     "exception": false,
     "start_time": "2025-12-29T18:04:39.524523",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **4. TEMPORAL CONSISTENCY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9542f295",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:04:39.545233Z",
     "iopub.status.busy": "2025-12-29T18:04:39.543680Z",
     "iopub.status.idle": "2025-12-29T18:05:12.300429Z",
     "shell.execute_reply": "2025-12-29T18:05:12.298737Z"
    },
    "papermill": {
     "duration": 32.769268,
     "end_time": "2025-12-29T18:05:12.305438",
     "exception": false,
     "start_time": "2025-12-29T18:04:39.536170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 4: TEMPORAL CONSISTENCY ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid datetime sequences found: 7,531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(These are trips where dropoff time <= pickup time)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of invalid records:\n",
      "\u001b[90m# A tibble: 10 × 5\u001b[39m\n",
      "   VendorID tpep_pickup_datetime tpep_dropoff_datetime trip_distance fare_amount\n",
      "   \u001b[3m\u001b[90m<fct>\u001b[39m\u001b[23m    \u001b[3m\u001b[90m<dttm>\u001b[39m\u001b[23m               \u001b[3m\u001b[90m<dttm>\u001b[39m\u001b[23m                        \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m 1        2020-02-25 \u001b[90m18:15:17\u001b[39m  2020-02-25 \u001b[90m18:15:17\u001b[39m               0         2.5\n",
      "\u001b[90m 2\u001b[39m 2        2020-01-26 \u001b[90m23:15:24\u001b[39m  2020-01-26 \u001b[90m23:15:24\u001b[39m               0        52  \n",
      "\u001b[90m 3\u001b[39m 1        2020-01-09 \u001b[90m12:52:41\u001b[39m  2020-01-09 \u001b[90m12:52:41\u001b[39m               0         2.5\n",
      "\u001b[90m 4\u001b[39m 1        2020-02-02 \u001b[90m18:35:30\u001b[39m  2020-02-02 \u001b[90m18:35:30\u001b[39m               0         6  \n",
      "\u001b[90m 5\u001b[39m 1        2020-01-13 \u001b[90m18:04:08\u001b[39m  2020-01-13 \u001b[90m18:04:08\u001b[39m               0         4.5\n",
      "\u001b[90m 6\u001b[39m 1        2020-01-07 \u001b[90m11:57:36\u001b[39m  2020-01-07 \u001b[90m11:57:36\u001b[39m               0         0  \n",
      "\u001b[90m 7\u001b[39m 1        2020-02-04 \u001b[90m21:03:15\u001b[39m  2020-02-04 \u001b[90m21:03:15\u001b[39m               0         4.5\n",
      "\u001b[90m 8\u001b[39m 1        2020-02-26 \u001b[90m07:59:40\u001b[39m  2020-02-26 \u001b[90m07:59:40\u001b[39m               0         2.5\n",
      "\u001b[90m 9\u001b[39m 1        2020-01-25 \u001b[90m23:00:06\u001b[39m  2020-01-25 \u001b[90m23:00:06\u001b[39m               0         3  \n",
      "\u001b[90m10\u001b[39m 1        2020-01-14 \u001b[90m19:01:41\u001b[39m  2020-01-14 \u001b[90m19:01:41\u001b[39m               0         6.5\n",
      "\n",
      "✓ Removed 7,531 records with invalid datetime sequences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating trip duration field ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trips with duration <= 1 minute found: 169,065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(These may be cancelled trips or recording errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Removed 169,065 short duration trips\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Remaining records: 11,648,618\n"
     ]
    }
   ],
   "source": [
    "cat(\"\\n--- STEP 4: TEMPORAL CONSISTENCY ---\\n\")\n",
    "\n",
    "# 4.1: Check for invalid datetime sequences\n",
    "# Find trips where dropoff time <= pickup time (logically impossible)\n",
    "df_invalid_dropoff <- df %>% \n",
    "  filter(tpep_dropoff_datetime <= tpep_pickup_datetime)\n",
    "\n",
    "total_invalid <- nrow(df_invalid_dropoff)\n",
    "\n",
    "cat(sprintf(\"Invalid datetime sequences found: %s\\n\", \n",
    "            format(total_invalid, big.mark=\",\")))\n",
    "cat(\"(These are trips where dropoff time <= pickup time)\\n\\n\")\n",
    "\n",
    "# Display 10 sample invalid records\n",
    "if(total_invalid > 0) {\n",
    "  cat(\"Sample of invalid records:\\n\")\n",
    "  sample_invalid <- df_invalid_dropoff %>% \n",
    "    slice_sample(n = min(10, total_invalid)) %>%\n",
    "    select(VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, \n",
    "           trip_distance, fare_amount)\n",
    "  print(sample_invalid, n = 10)\n",
    "  \n",
    "  # Remove invalid records\n",
    "  df <- df %>% anti_join(df_invalid_dropoff, by = names(df))\n",
    "  cat(sprintf(\"\\n✓ Removed %s records with invalid datetime sequences\\n\", \n",
    "              format(total_invalid, big.mark=\",\")))\n",
    "}\n",
    "\n",
    "# 4.2: Create trip duration field and filter unrealistic short trips\n",
    "cat(\"\\n--- Creating trip duration field ---\\n\")\n",
    "\n",
    "df <- df %>% \n",
    "  mutate(\n",
    "    trip_duration = round(as.numeric(\n",
    "      difftime(\n",
    "        time1 = tpep_dropoff_datetime, \n",
    "        time2 = tpep_pickup_datetime,\n",
    "        units = 'mins'\n",
    "      )\n",
    "    ))\n",
    "  )\n",
    "\n",
    "# Find trips with duration <= 1 minute\n",
    "# These are likely errors or cancelled trips\n",
    "df_short_duration <- df %>% \n",
    "  filter(trip_duration <= 1)\n",
    "\n",
    "total_short_duration <- nrow(df_short_duration)\n",
    "\n",
    "cat(sprintf(\"Trips with duration <= 1 minute found: %s\\n\", \n",
    "            format(total_short_duration, big.mark=\",\")))\n",
    "cat(\"(These may be cancelled trips or recording errors)\\n\")\n",
    "\n",
    "# Remove short duration trips\n",
    "before_short_removal <- nrow(df)\n",
    "df <- df %>% anti_join(df_short_duration, by = names(df))\n",
    "after_short_removal <- nrow(df)\n",
    "\n",
    "cat(sprintf(\"✓ Removed %s short duration trips\\n\", \n",
    "            format(before_short_removal - after_short_removal, big.mark=\",\")))\n",
    "cat(sprintf(\"✓ Remaining records: %s\\n\", format(nrow(df), big.mark=\",\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dec471",
   "metadata": {
    "papermill": {
     "duration": 0.005656,
     "end_time": "2025-12-29T18:05:12.317002",
     "exception": false,
     "start_time": "2025-12-29T18:05:12.311346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **5. TEMPORAL FEATURE ENGINEERING**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54028405",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:05:12.332755Z",
     "iopub.status.busy": "2025-12-29T18:05:12.331184Z",
     "iopub.status.idle": "2025-12-29T18:06:19.962450Z",
     "shell.execute_reply": "2025-12-29T18:06:19.960445Z"
    },
    "papermill": {
     "duration": 67.642525,
     "end_time": "2025-12-29T18:06:19.965342",
     "exception": false,
     "start_time": "2025-12-29T18:05:12.322817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 5: TEMPORAL FEATURE ENGINEERING ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating time-based features for analysis...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created: year_pickup (e.g., 2020)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created: pickup_quarter_label (e.g., 2020.1 for Q1 2020)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created: pickup_year_semester (e.g., 20201 for first semester)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created: pickup_month (e.g., JAN, FEB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created: pickup_weekday (e.g., MON, TUE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created: am_pm (AM for midnight-11:59am, PM for noon-11:59pm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created: is_holiday (TRUE/FALSE flag for NYSE holidays)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Total temporal features created: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Final dataset dimensions: 11,648,618 rows × 26 columns\n"
     ]
    }
   ],
   "source": [
    "cat(\"\\n--- STEP 5: TEMPORAL FEATURE ENGINEERING ---\\n\")\n",
    "cat(\"Creating time-based features for analysis...\\n\\n\")\n",
    "\n",
    "# Extract year from pickup datetime\n",
    "df <- df %>%\n",
    "  mutate(year_pickup = year(tpep_pickup_datetime))\n",
    "\n",
    "cat(\"✓ Created: year_pickup (e.g., 2020)\\n\")\n",
    "\n",
    "# Add quarter with year (format: 2020.1, 2020.2, etc.)\n",
    "df <- df %>% \n",
    "  mutate(pickup_quarter_label = quarter(tpep_pickup_datetime, type = \"year.quarter\"))\n",
    "\n",
    "cat(\"✓ Created: pickup_quarter_label (e.g., 2020.1 for Q1 2020)\\n\")\n",
    "\n",
    "# Add semester with year (1 = Jan-Jun, 2 = Jul-Dec)\n",
    "df <- df %>% \n",
    "  mutate(pickup_year_semester = semester(tpep_pickup_datetime, with_year = TRUE))\n",
    "\n",
    "cat(\"✓ Created: pickup_year_semester (e.g., 20201 for first semester)\\n\")\n",
    "\n",
    "# Add month abbreviation in uppercase (JAN, FEB, MAR, etc.)\n",
    "df <- df %>% \n",
    "  mutate(pickup_month = toupper(month(tpep_pickup_datetime, label = TRUE)))\n",
    "\n",
    "cat(\"✓ Created: pickup_month (e.g., JAN, FEB)\\n\")\n",
    "\n",
    "# Add day of week abbreviation in uppercase (MON, TUE, WED, etc.)\n",
    "df$pickup_weekday <- toupper(wday(df$tpep_pickup_datetime, label = TRUE))\n",
    "\n",
    "cat(\"✓ Created: pickup_weekday (e.g., MON, TUE)\\n\")\n",
    "\n",
    "# Add AM/PM indicator\n",
    "df$am_pm <- ifelse(am(df$tpep_pickup_datetime), \"AM\", \"PM\")\n",
    "\n",
    "cat(\"✓ Created: am_pm (AM for midnight-11:59am, PM for noon-11:59pm)\\n\")\n",
    "\n",
    "# Add holiday flag using NYSE holidays as proxy for NYC holidays\n",
    "# Note: This includes major holidays like New Year's, Martin Luther King Day, etc.\n",
    "holidays <- as.Date(holidayNYSE(2020))\n",
    "df$is_holiday <- as.Date(df$tpep_pickup_datetime) %in% holidays\n",
    "\n",
    "cat(\"✓ Created: is_holiday (TRUE/FALSE flag for NYSE holidays)\\n\")\n",
    "\n",
    "cat(sprintf(\"\\n✓ Total temporal features created: 7\\n\"))\n",
    "cat(sprintf(\"✓ Final dataset dimensions: %s rows × %s columns\\n\", \n",
    "            format(nrow(df), big.mark=\",\"), \n",
    "            ncol(df)))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4707625,
     "sourceId": 7995697,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30749,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "r",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 203.244747,
   "end_time": "2025-12-29T18:06:20.298821",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-29T18:02:57.054074",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
